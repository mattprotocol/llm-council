--- a/mcp_server/src/services/factories.py
+++ b/mcp_server/src/services/factories.py
@@ -20,6 +20,13 @@ from graphiti_core.llm_client import LLMClient, OpenAIClient
 from graphiti_core.llm_client.config import LLMConfig as GraphitiLLMConfig
 
+# Import OpenAIGenericClient for local LLM support
+try:
+    from graphiti_core.llm_client.openai_generic_client import OpenAIGenericClient
+    HAS_OPENAI_GENERIC = True
+except ImportError:
+    HAS_OPENAI_GENERIC = False
+
+# Import custom LMStudio client
+try:
+    from lmstudio_client import LMStudioClient
+    HAS_LMSTUDIO = True
+except ImportError:
+    HAS_LMSTUDIO = False
+
 # Try to import additional providers if available
 try:
     from graphiti_core.embedder.azure_openai import AzureOpenAIEmbedderClient
@@ -130,6 +143,52 @@ class LLMClientFactory:
                     return OpenAIClient(config=llm_config, reasoning=None, verbosity=None)
 
+            case 'openai_generic':
+                # For local LLM servers (LM Studio, Ollama, etc.) that support OpenAI API
+                if not HAS_OPENAI_GENERIC:
+                    raise ValueError(
+                        'OpenAIGenericClient not available in current graphiti-core version'
+                    )
+                
+                # Get generic provider config
+                if hasattr(config.providers, 'openai_generic') and config.providers.openai_generic:
+                    generic_config = config.providers.openai_generic
+                    api_key = generic_config.api_key or 'not-needed'
+                    base_url = generic_config.api_url
+                else:
+                    import os
+                    api_key = os.environ.get('OPENAI_API_KEY', 'not-needed')
+                    base_url = os.environ.get('OPENAI_BASE_URL', 'http://localhost:1234/v1')
+                
+                logger.info(f'Creating OpenAI Generic client with base_url: {base_url}')
+                
+                llm_config = GraphitiLLMConfig(
+                    api_key=api_key,
+                    base_url=base_url,
+                    model=config.model,
+                    temperature=config.temperature,
+                    max_tokens=config.max_tokens or 16384,
+                )
+                return OpenAIGenericClient(config=llm_config, max_tokens=config.max_tokens or 16384)
+
+            case 'lmstudio':
+                # For LM Studio with strict JSON schema support
+                if not HAS_LMSTUDIO:
+                    raise ValueError('LMStudioClient not available')
+                
+                if hasattr(config.providers, 'openai_generic') and config.providers.openai_generic:
+                    generic_config = config.providers.openai_generic
+                    api_key = generic_config.api_key or 'not-needed'
+                    base_url = generic_config.api_url
+                else:
+                    import os
+                    api_key = os.environ.get('OPENAI_API_KEY', 'not-needed')
+                    base_url = os.environ.get('OPENAI_BASE_URL', 'http://localhost:1234/v1')
+                
+                logger.info(f'Creating LM Studio client with base_url: {base_url}')
+                
+                llm_config = GraphitiLLMConfig(
+                    api_key=api_key,
+                    base_url=base_url,
+                    model=config.model,
+                    temperature=config.temperature,
+                    max_tokens=config.max_tokens or 16384,
+                )
+                return LMStudioClient(config=llm_config, max_tokens=config.max_tokens or 16384)
+
             case 'azure_openai':
                 # ... rest of the file unchanged
